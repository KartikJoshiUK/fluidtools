# FluidTools Environment Configuration Template
# Copy this file to .env and fill in your values

# ============================================
# LLM Provider Configuration
# ============================================

# Provider type: openai, anthropic, gemini, or ollama
PROVIDER_TYPE=openai

# Model name (provider-specific)
# OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# Gemini: gemini-2.5-flash-lite, gemini-pro
# Ollama: llama3.2:3b, mistral, etc.
PROVIDER_MODEL=gpt-4

# Temperature (0.0 - 2.0 for OpenAI, 0.0 - 1.0 for Anthropic/Gemini)
PROVIDER_TEMPERATURE=0.7

# Optional: Maximum tokens to generate
# PROVIDER_MAX_TOKENS=1000

# Optional: Top-p sampling (0.0 - 1.0)
# PROVIDER_TOP_P=0.9

# ============================================
# Provider API Keys (use only what you need)
# ============================================

# OpenAI API Key
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Anthropic API Key
# Get yours at: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google Gemini API Key
# Get yours at: https://makersuite.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-api-key-here

# ============================================
# Ollama Configuration (for local models)
# ============================================

# Ollama base URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama context window size
# OLLAMA_NUM_CTX=2048

# ============================================
# Application Configuration
# ============================================

# API authentication token for your APIs
# This is used when calling your Postman collection APIs
ACCESS_TOKEN=your-api-access-token-here



LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=fluidtools
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
